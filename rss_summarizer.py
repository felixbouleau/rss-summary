#!/usr/bin/env python3
"""
RSS Feed Summarizer using Claude AI

This script fetches RSS feed content from a provided URL, processes entries from the last 24 hours,
and uses Anthropic's Claude AI to generate a summary of the content.
"""

import os
import sys
import time
import datetime
import feedparser
import yaml
from dateutil import parser as date_parser
from anthropic import Anthropic
from feedgen.feed import FeedGenerator # Add feedgen import
import http.server
import socketserver
import threading
import functools # For http server directory binding
import logging # Add logging import

def get_recent_entries(feed_url):
    """
    Fetch entries from the RSS feed published within a configurable lookback period.
    The lookback period (in hours) is set by the RSS_LOOKBACK_HOURS env var, defaulting to 24.
    """
    # Get lookback hours from env var, default to 24
    try:
        lookback_hours = int(os.environ.get("RSS_LOOKBACK_HOURS", 24))
        if lookback_hours <= 0:
            logging.warning("RSS_LOOKBACK_HOURS must be positive. Using default 24 hours.")
            lookback_hours = 24
    except ValueError:
        logging.warning("Invalid RSS_LOOKBACK_HOURS value. Using default 24 hours.")
        lookback_hours = 24

    # Log the attempt
    logging.debug(f"Attempting to fetch entries from the last {lookback_hours} hours for: {feed_url}")

    try:
        feed = feedparser.parse(feed_url)
        # Note: feedparser usually returns an empty list, not None or error on no entries
        # if not feed.entries:
        #     logging.warning(f"No entries found in feed: {feed_url}")
        #     # Don't exit here, just return empty list later
        # Get current time and calculate cutoff based on lookback_hours
        now = datetime.datetime.now(datetime.timezone.utc)
        cutoff = now - datetime.timedelta(hours=lookback_hours)

        # Filter entries published after the cutoff time
        recent_entries = []
        for entry in feed.entries:
            if hasattr(entry, 'published'):
                try:
                    pub_date = date_parser.parse(entry.published)
                    if pub_date > cutoff:
                        recent_entries.append(entry)
                except Exception as e:
                    logging.warning(f"Could not parse date for entry: {e}")

        # Log the count of filtered entries before returning
        logging.info(f"Fetched {len(recent_entries)} items from the last {lookback_hours} hours for feed: {feed_url}")
        return recent_entries
    except Exception as e:
        logging.error(f"Error fetching or parsing RSS feed for {feed_url}: {e}")
        # Return empty list instead of exiting, so other feeds can be tried
        return [] 

def load_feeds_from_yaml():
    """
    Load feed URLs from a YAML file specified by the RSS_FEEDS_CONFIG env var,
    defaulting to 'feeds.yml'.
    """
    # Get config file path from env var, default to 'feeds.yml'
    filepath = os.environ.get("RSS_FEEDS_CONFIG", "feeds.yml")
    logging.info(f"Loading feeds from: {filepath}")

    try:
        with open(filepath, 'r') as f:
            config = yaml.safe_load(f)
            if config and 'feeds' in config and isinstance(config['feeds'], list):
                # Extract URLs from the list of dictionaries
                urls = [feed.get('url') for feed in config['feeds'] if isinstance(feed, dict) and 'url' in feed]
                if not urls:
                    logging.error(f"No valid feed URLs found in {filepath}")
                    sys.exit(1)
                return urls
            else:
                logging.error(f"Invalid format in {filepath}. Expected a 'feeds' list with 'url' keys.")
                sys.exit(1)
    except FileNotFoundError:
        logging.error(f"Configuration file {filepath} not found.")
        sys.exit(1)
    except yaml.YAMLError as e:
        logging.error(f"Error parsing YAML file {filepath}: {e}")
        sys.exit(1)
    except Exception as e:
        logging.error(f"An unexpected error occurred while loading feeds: {e}")
        sys.exit(1)

# --- RSS Feed Generation ---

def generate_rss_feed(summary_text, feed_file_path):
    """
    Generates an RSS feed file, prepending the new summary to existing entries.
    """
    fg = FeedGenerator()
    feed_abspath = os.path.abspath(feed_file_path)
    feed_link = f"file://{feed_abspath}" # Default link

    # --- Default Feed Metadata ---
    default_title = 'AI Generated Feed Summary'
    default_desc = 'Daily summary of RSS feeds generated by Claude AI.'
    default_lang = 'en'

    # --- Add the NEW entry first ---
    fe_new = fg.add_entry(order='prepend') # Add new entry at the beginning
    now = datetime.datetime.now(datetime.timezone.utc)
    entry_title = f"Summary for {now.strftime('%Y-%m-%d %H:%M:%S UTC')}"
    fe_new.title(entry_title)
    fe_new.id(f"urn:uuid:{now.isoformat()}") # Unique ID for the new entry
    fe_new.link(href=feed_link) # Link entry back to the feed itself (can be improved later)
    fe_new.content(summary_text, type='html')
    fe_new.pubDate(now)

    # --- Load existing entries if feed file exists ---
    parsed_feed = None
    if os.path.exists(feed_file_path):
        try:
            logging.info(f"Loading existing feed from: {feed_file_path}")
            parsed_feed = feedparser.parse(feed_file_path)
            if parsed_feed.bozo:
                 # Error during parsing
                 raise ValueError(f"Feed parsing error: {parsed_feed.bozo_exception}")
        except Exception as e:
            logging.warning(f"Could not parse existing feed file '{feed_file_path}'. A new feed will be created. Error: {e}")
            parsed_feed = None # Reset on error

    # --- Set Feed Metadata (from parsed feed or defaults) ---
    if parsed_feed and parsed_feed.feed:
        feed_title = parsed_feed.feed.get('title', default_title)
        # Try getting the 'alternate' link first
        feed_links = parsed_feed.feed.get('links', [])
        feed_link_href = next((link.get('href') for link in feed_links if link.get('rel') == 'alternate'), None)
        if feed_link_href is None: # Fallback to the main link if no alternate
             feed_link_href = parsed_feed.feed.get('link', feed_link)

        feed_desc = parsed_feed.feed.get('description', default_desc)
        feed_lang = parsed_feed.feed.get('language', default_lang)
        
        fg.title(feed_title)
        fg.link(href=feed_link_href, rel='alternate')
        fg.description(feed_desc)
        fg.language(feed_lang)

        # --- Add OLD entries ---
        logging.info(f"Adding {len(parsed_feed.entries)} existing entries.")
        for entry in parsed_feed.entries:
            fe_old = fg.add_entry(order='append') # Append old entries
            fe_old.title(entry.get('title', ''))
            fe_old.id(entry.get('id', entry.get('link', ''))) # Use id or link as fallback
            fe_old.link(href=entry.get('link', ''))

            # Handle content vs summary vs description, prefer content
            content_detail = entry.get('content')
            if content_detail:
                 # feedparser returns a list for content, usually with one item
                 content_value = content_detail[0].get('value')
                 content_type = content_detail[0].get('type', 'text/plain')
                 # Map common types to what feedgen expects
                 if 'html' in content_type:
                     fe_old.content(content_value, type='html')
                 else:
                     fe_old.content(content_value, type='text')
            elif entry.get('summary'):
                 fe_old.summary(entry.get('summary')) # feedgen uses summary()
            elif entry.get('description'):
                 fe_old.description(entry.get('description')) # feedgen uses description()

            # Use published_parsed or updated_parsed from feedparser
            pub_date_parsed = entry.get('published_parsed') or entry.get('updated_parsed')
            if pub_date_parsed:
                 # Convert struct_time to datetime
                 dt_aware = datetime.datetime.fromtimestamp(time.mktime(pub_date_parsed), tz=datetime.timezone.utc)
                 fe_old.pubDate(dt_aware)
            elif entry.get('published'): # Fallback to parsing string if parsed version not available
                 try:
                     fe_old.pubDate(date_parser.parse(entry.get('published')))
                 except Exception:
                     pass # Ignore if date parsing fails

    else:
        # Set default metadata if feed didn't exist or parsing failed
        fg.title(default_title)
        fg.link(href=feed_link, rel='alternate')
        fg.description(default_desc)
        fg.language(default_lang)


    try:
        # Ensure the output directory exists
        os.makedirs(os.path.dirname(feed_file_path), exist_ok=True)
        # Generate the RSS feed file
        fg.rss_file(feed_file_path, pretty=True)
        logging.info(f"RSS feed updated successfully: {feed_file_path}")
    except Exception as e:
        logging.error(f"Error writing RSS feed file: {e}")


# --- HTTP Server ---

def start_http_server(directory, port):
    """Starts a simple HTTP server in a background thread."""
    Handler = functools.partial(http.server.SimpleHTTPRequestHandler, directory=directory)
    # Suppress standard request logging from SimpleHTTPRequestHandler
    Handler.log_message = lambda *args: None 
    httpd = socketserver.TCPServer(("", port), Handler)

    logging.info(f"Serving RSS feed from directory '{directory}' on port {port}")
    logging.info(f"Feed URL: http://localhost:{port}/feed.xml") # Assuming feed is named feed.xml

    # Run the server in a separate thread
    server_thread = threading.Thread(target=httpd.serve_forever)
    server_thread.daemon = True # Allows program to exit even if thread is running
    server_thread.start()


# --- Core Logic ---

def format_entries_for_prompt(entries):
    """
    Format RSS entries into a text format for the prompt.
    """
    formatted_text = ""
    
    for i, entry in enumerate(entries, 1):
        formatted_text += f"POST {i}:\n"
        formatted_text += f"Title: {entry.title}\n"
        formatted_text += f"Link: {entry.link}\n"
        
        if hasattr(entry, 'published'):
            formatted_text += f"Published: {entry.published}\n"
            
        if hasattr(entry, 'summary'):
            formatted_text += f"Summary: {entry.summary}\n"
        elif hasattr(entry, 'description'):
            formatted_text += f"Description: {entry.description}\n"
            
        formatted_text += "\n---\n\n"
    
    return formatted_text

def get_prompt():
    """
    Read the prompt from the prompt.txt file.
    """
    try:
        with open("prompt.txt", "r") as f:
            return f.read()
    except Exception as e:
        logging.error(f"Error reading prompt file: {e}")
        sys.exit(1)

def summarize_with_claude(entries_text):
    """
    Use Claude API to summarize the entries.
    """
    api_key = os.environ.get("ANTHROPIC_API_KEY")
    if not api_key:
        logging.error("ANTHROPIC_API_KEY environment variable not set")
        sys.exit(1)

    client = Anthropic(api_key=api_key)
    prompt_text = get_prompt()

    try:
        message = client.messages.create(
            model="claude-3-5-haiku-20241022",
            max_tokens=4096, # Increase token limit
            messages=[
                {
                    "role": "user",
                    "content": f"{prompt_text}\n\nHere are the Reddit posts from the last 24 hours:\n\n{entries_text}"
                }
            ]
        )
        logging.info("Successfully received summary from Claude API.")
        return message.content[0].text
    except Exception as e:
        logging.error(f"Error calling Claude API: {e}")
        # Don't exit the whole script on API error, just return None
        # sys.exit(1)
        return None

def run_summary_cycle(feed_file_path):
    """
    Performs one cycle of fetching, summarizing, and saving the feed.
    """
    logging.info(f"--- Starting summary cycle at {datetime.datetime.now()} ---")
    feed_urls = load_feeds_from_yaml()

    if not feed_urls:
        logging.warning("No feed URLs loaded. Skipping cycle.")
        return

    all_entries = []
    # Get recent entries for each feed based on the lookback window
    for feed_url in feed_urls:
        # Fetch entries first using the updated function
        entries = get_recent_entries(feed_url)
        # Log the result after fetching (message now includes lookback period)
        # print(f"Fetched {len(entries)} items from feed: {feed_url}") # Log is now inside get_recent_entries
        if entries:
            all_entries.extend(entries)
        # No need for an else here, the count in the log message indicates 0 entries

    if not all_entries:
        logging.info("No new entries found across all feeds in the lookback period.")
        # Don't exit, just skip generating a summary for this cycle
        return

    # Sort entries by published date (newest first)
    all_entries.sort(key=lambda x: date_parser.parse(x.published) if hasattr(x, 'published') else datetime.datetime.min.replace(tzinfo=datetime.timezone.utc), reverse=True)

    # Format entries for the prompt
    entries_text = format_entries_for_prompt(all_entries)
    
    # Get summary from Claude
    summary = summarize_with_claude(entries_text)

    if summary:
        # Generate the RSS feed file
        generate_rss_feed(summary, feed_file_path)
    else:
        logging.error("Failed to generate summary from Claude. Skipping feed update.")

    logging.info(f"--- Summary cycle finished at {datetime.datetime.now()} ---")


def main():
    """
    Main function to set up the server and run the summary loop.
    """
    # Configuration from environment variables
    output_dir = os.environ.get("RSS_OUTPUT_DIR", "./rss")
    server_port = int(os.environ.get("RSS_SERVER_PORT", 8080))
    refresh_interval = int(os.environ.get("RSS_REFRESH_INTERVAL", 86400)) # Default 24 hours
    feed_filename = "feed.xml"
    feed_file_path = os.path.join(output_dir, feed_filename)

    # --- Configure Logging ---
    logging.basicConfig(level=logging.INFO, 
                        format='%(asctime)s - %(levelname)s - %(message)s',
                        datefmt='%Y-%m-%d %H:%M:%S')

    logging.info("--- RSS Summarizer Service Starting ---")
    logging.info(f"Output Directory: {os.path.abspath(output_dir)}")
    logging.info(f"Server Port: {server_port}")
    logging.info(f"Refresh Interval: {refresh_interval} seconds")
    logging.info(f"Feed File: {feed_file_path}")
    logging.info("---------------------------------------")

    # Ensure output directory exists
    try:
        os.makedirs(output_dir, exist_ok=True)
    except Exception as e:
        logging.error(f"Error creating output directory '{output_dir}': {e}")
        sys.exit(1)

    # Start the HTTP server in a background thread
    start_http_server(output_dir, server_port)

    # Run the summary generation loop
    while True:
        try:
            run_summary_cycle(feed_file_path)
            logging.info(f"Sleeping for {refresh_interval} seconds until the next cycle...")
            time.sleep(refresh_interval)
        except KeyboardInterrupt:
            logging.info("Shutdown requested. Exiting.")
            sys.exit(0)
        except Exception as e:
            logging.error(f"An unexpected error occurred in the main loop: {e}", exc_info=True) # Log traceback
            logging.info(f"Will retry after {refresh_interval} seconds.")
            time.sleep(refresh_interval)


if __name__ == "__main__":
    main()
